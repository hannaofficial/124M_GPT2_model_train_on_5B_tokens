# GPT2_train_on_5B_tokens

124 Million parameter GPT2 model <br/>
This GPT2 model is build from scratch <br/>
Dataset used: Fineweb edu 10B tokens <br/>
