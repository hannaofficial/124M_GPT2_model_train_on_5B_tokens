# GPT2_train_on_5B_tokens

124 Million parameter GPT2 model
This GPT2 model is build from scratch
Dataset used: Fineweb edu 10B tokens
